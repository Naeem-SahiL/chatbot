# -*- coding: utf-8 -*-
"""Model Training (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dphkH9SZqm1bT3KwfntL88SjMs7V0sv-
"""

# @title
!pip install pypdf
!pip install python-dotenv
!pip install -q transformers einops accelerate langchain bitsandbytes
!pip install -q sentence_transformers
!pip install llama-index
!pip install llama-index-llms-huggingface
!pip install -U langchain-community
!pip install llama-index-embeddings-langchain
!pip install Flask pyngrok

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

!huggingface-cli login

import logging
import sys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext

documents = SimpleDirectoryReader("/content/data").load_data()

from llama_index.llms.huggingface import HuggingFaceLLM

# from llama_index.prompts.prompts import SimpleInputPrompt


system_prompt = "You are an advanced Q&A assistant designed to provide detailed and accurate explanations based on the context of the provided documents, which include Software Requirements Specifications (SRS), the Codebase, and Database Models."



# This will wrap the default prompts that are internal to llama-index
# for 0.7.6
#query_wrapper_prompt = SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")
# for latest
#query_wrapper_prompt = f"<|USER|>{query_str}<|ASSISTANT|>"
#or
query_wrapper_prompt = "<|USER|>{query_str}<|ASSISTANT|>"

import torch

llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
    model_name="meta-llama/Llama-2-7b-chat-hf",
    device_map="auto",
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={"torch_dtype": torch.float16 , "load_in_8bit":True}
)

from langchain.embeddings import HuggingFaceEmbeddings
from llama_index.core import ServiceContext

embed_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

service_context = ServiceContext.from_defaults(
    chunk_size=1024,
    llm=llm,
    embed_model=embed_model
)

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

query_engine = index.as_query_engine()

while True:
  query=input()
  response = query_engine.query(query)
  print(response)

!pip install flask-cors

!pip install Flask pyngrok

# Set up and start Flask app
from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok, conf
from datetime import datetime


# Set your ngrok authentication token
NGROK_AUTH_TOKEN = "2i8wDbAaoh7XyDFxi0nSvrULr8r_4GVvkwWYJhkCGsZG1MS1B"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
conf.get_default().auth_token = NGROK_AUTH_TOKEN

app = Flask(__name__)
CORS(app)
@app.route("/promptHandling", methods=["POST"])
def promptHandling():
   data = request.json
   prompt = data.get("prompt")
   print(prompt)
   if prompt:
    response = query_engine.query(prompt)
    # current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # # Format the response string
    # response = f"Reply to '{prompt}' at {current_time}"
    return jsonify({"message": f"{response}!"})

   else:
    return jsonify({"error": "prompt not provided"}), 400

   return jsonify({"error": "No input data provided"}), 400

# Start ngrok tunnel
public_url = ngrok.connect(5000,"http")
print(f"ngrok tunnel created: {public_url}")

# Run Flask application
app.run()